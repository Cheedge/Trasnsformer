{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "listed-controversy",
   "metadata": {},
   "source": [
    "# build data pipline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-plaintiff",
   "metadata": {},
   "source": [
    "# Seq to Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "still-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import spacy\n",
    "import random\n",
    "import io\n",
    "from collections import Counter\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-tower",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "distributed-honey",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.utils import extract_archive\n",
    "path = '/home/sharma/Desktop/DeepLearning/Testing/Datasets/multi30k-dataset/data/task1/raw/'\n",
    "train_files = ('train.de.gz', 'train.en.gz')\n",
    "val_files = ('val.de.gz', 'val.en.gz')\n",
    "test_files = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
    "\n",
    "train_filepaths = [extract_archive(path + file)[0] for file in train_files]\n",
    "val_filepaths = [extract_archive(path + file)[0] for file in val_files]\n",
    "test_filepaths = [extract_archive(path + file)[0] for file in test_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sexual-partner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence # for padding batch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import Counter\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "import io # for io.open\n",
    "import torch\n",
    "\n",
    "def build_vocab(filepath, tokenizer, min_freq):\n",
    "    counter = Counter()\n",
    "    with io.open(filepath) as f:\n",
    "        for s_ in f:\n",
    "            counter.update(tokenizer(s_))\n",
    "    return Vocab(counter, specials=['<UNK>', '<PAD>', '<BOS>', '<EOS>'], min_freq=min_freq)\n",
    "\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, pathfilename, Vocabulary, tokenizer, freq_threshold=5):\n",
    "        self.path = path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.txt = open(pathfilename, 'r').read().split('\\n')\n",
    "        self.vocab = Vocabulary\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.txt)\n",
    "    \n",
    "\n",
    "    def textnumericalizer(self, text):\n",
    "        numerical_tok = self.tokenizer(text.lower())\n",
    "        numerical_sen = []\n",
    "        for tok in numerical_tok:\n",
    "            if tok not in self.vocab.stoi:\n",
    "                self.vocab.stoi[tok] = 0\n",
    "            numerical_sen.append(self.vocab.stoi[tok])\n",
    "        return numerical_sen\n",
    "    \n",
    "    # get a numeralized and format sentence\n",
    "    # as \"es ist ein Ei\" -> tensor([2, 439, 72, 16, 0, 3])\n",
    "    def __getitem__(self, batch_idx):\n",
    "        sentence = self.txt[batch_idx]\n",
    "        sen_format = [self.vocab['<BOS>']]\n",
    "        sen_format += self.textnumericalizer(sentence)\n",
    "        sen_format.append(self.vocab['<EOS>'])\n",
    "\n",
    "        return torch.tensor(sen_format)\n",
    "    \n",
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        targets = []\n",
    "        for idx in batch:\n",
    "            targets.append(idx)\n",
    "        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n",
    "\n",
    "        return targets\n",
    "    \n",
    "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "wrong-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_train_voc = build_vocab(train_filepaths[0], de_tokenizer, 2)\n",
    "en_train_voc = build_vocab(train_filepaths[1], en_tokenizer, 2)\n",
    "de_val_voc = build_vocab(val_filepaths[0], de_tokenizer, 2)\n",
    "en_val_voc = build_vocab(val_filepaths[1], en_tokenizer, 2)\n",
    "de_test_voc = build_vocab(test_filepaths[0], de_tokenizer, 2)\n",
    "en_test_voc = build_vocab(test_filepaths[1], en_tokenizer, 2)\n",
    "\n",
    "train_de_dataset = TextDataset(train_filepaths[0], de_test_voc, de_tokenizer)\n",
    "train_en_dataset = TextDataset(train_filepaths[1], en_test_voc, en_tokenizer)\n",
    "val_de_dataset = TextDataset(val_filepaths[0], de_test_voc, de_tokenizer)\n",
    "val_en_dataset = TextDataset(val_filepaths[1], en_test_voc, en_tokenizer)\n",
    "test_de_dataset = TextDataset(test_filepaths[0], de_test_voc, de_tokenizer)\n",
    "test_en_dataset = TextDataset(test_filepaths[1], en_test_voc, en_tokenizer)\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "pad_idx = test_de_dataset.vocab.stoi['<PAD>']\n",
    "\n",
    "train_en_loader = DataLoader(dataset=train_en_dataset, \n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True, pin_memory=True, \n",
    "                    collate_fn=MyCollate(pad_idx=pad_idx)\n",
    "                    )\n",
    "train_de_loader = DataLoader(dataset=train_de_dataset, \n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True, pin_memory=True, \n",
    "                    collate_fn=MyCollate(pad_idx=pad_idx)\n",
    "                   )\n",
    "\n",
    "val_en_loader = DataLoader(dataset=val_en_dataset, \n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True, pin_memory=True, \n",
    "                    collate_fn=MyCollate(pad_idx=pad_idx)\n",
    "                    )\n",
    "val_de_loader = DataLoader(dataset=val_de_dataset, \n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True, pin_memory=True, \n",
    "                    collate_fn=MyCollate(pad_idx=pad_idx)\n",
    "                   )\n",
    "\n",
    "text_en_loader = DataLoader(dataset=test_en_dataset, \n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True, pin_memory=True, \n",
    "                    collate_fn=MyCollate(pad_idx=pad_idx)\n",
    "                    )\n",
    "text_de_loader = DataLoader(dataset=test_de_dataset, \n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True, pin_memory=True, \n",
    "                    collate_fn=MyCollate(pad_idx=pad_idx)\n",
    "                   )\n",
    "#print(next(iter(train_de_loader)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-antigua",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "least-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_batch, seq_len, embed_dim, hidden_size, num_layers, p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed = nn.Embedding(seq_len, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers, dropout = p, batch_first=False)\n",
    "        self.drop = nn.Dropout(p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: S, N\n",
    "        x_hat = self.drop(self.embed(x))\n",
    "        # x_hat: S, N, E\n",
    "        h_t, (h_f, c0) = self.lstm(x_hat)\n",
    "        # h_t, h0, c0: S, N, hidden_size\n",
    "        return h_t, h_f, c0\n",
    "                \n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_batch, seq_len, embed_dim, hidden_size, num_class, num_layers, p):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = nn.Embedding(seq_len, embed_dim)\n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers, dropout=p, batch_first=False)\n",
    "        self.linear = nn.Linear(hidden_size, num_class)\n",
    "        \n",
    "    def forward(self, x, h0, c0):\n",
    "        # x: 1, N. means every time feed into one word of N batchs\n",
    "\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        x_t = self.drop(self.embed(x))\n",
    "\n",
    "        # x_t: 1, N, E\n",
    "        h_t, (h_f, c0) = self.lstm(x_t, (h0, c0))\n",
    "        # h_t, h0, c0: 1, N, hidden_size\n",
    "        #x_t, h0, c0 = h_t, hn, cn\n",
    "        out = self.linear(h_t)\n",
    "\n",
    "        return out, h_f, c0\n",
    "        \n",
    "        \n",
    "        \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, src, tgt, teacher_force_ratio=0.5):\n",
    "        src_len, num_batch = src.shape\n",
    "        tgt_len = tgt.shape[0]\n",
    "        tgt_vocab_size = len(en_train_voc)\n",
    "        \n",
    "        pred = torch.zeros(tgt_len, num_batch, tgt_vocab_size, requires_grad=True).to(device)\n",
    "        \n",
    "        h_t, h_f, c0 = self.encoder(src)\n",
    "        \n",
    "        # grab start token\n",
    "        x = tgt[0]\n",
    "        \n",
    "        for t in range(1, tgt_len):\n",
    "\n",
    "            output, h_f, c0 = self.decoder(x, h_f, c0)\n",
    "            # 1, N, num_class\n",
    "            \n",
    "            best_pred = output.argmax(dim=2)\n",
    "            best_pred = torch.squeeze(best_pred)\n",
    "#             print('bestpre', best_pred.shape)\n",
    "#             print('tgt_t', tgt[t].shape)\n",
    "            \n",
    "            x = tgt[t] if random.random() < teacher_force_ratio else best_pred\n",
    "            \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-porcelain",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "royal-fossil",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
    "# training hyperparameters\n",
    "num_epoch = 20\n",
    "lr = 0.001\n",
    "#batch_size=64\n",
    "\n",
    "# Model hyperparameters\n",
    "load_model = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seq_len_encoder = len(de_train_voc)\n",
    "seq_len_decoder = len(en_train_voc)\n",
    "output_size = len(en_train_voc)\n",
    "encoder_embedding_size = 300\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 1024  # Needs to be the same for both RNN's\n",
    "num_layers = 2\n",
    "enc_drop = 0.5\n",
    "dec_drop = 0.5\n",
    "\n",
    "# Tensorboard to get nice loss plot\n",
    "writer = SummaryWriter(f\"runs/loss_plot\")\n",
    "step = 0\n",
    "\n",
    "\n",
    "my_encoder = Encoder(batch_size, seq_len_encoder, encoder_embedding_size, hidden_size, num_layers, enc_drop)\n",
    "my_decoder = Decoder(batch_size, seq_len_decoder, decoder_embedding_size, hidden_size, \n",
    "                     output_size, num_layers, dec_drop)\n",
    "\n",
    "model = Seq2Seq(my_encoder, my_decoder)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "sentence = \"ein boot mit mehreren männern darauf wird von einem großen pferdegespann ans ufer gezogen.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "descending-foundation",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 / 20]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-099f0de0fb8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtarget_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_sentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-074d19295ee7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, teacher_force_ratio)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0;31m# 1, N, num_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m#             pred[t] = output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-074d19295ee7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, h0, c0)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# h_t, h0, c0: 1, N, hidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m#x_t, h0, c0 = h_t, hn, cn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    \n",
    "    print(f\"[Epoch {epoch} / {num_epoch}]\")\n",
    "\n",
    "#     model.eval()\n",
    "\n",
    "#     translated_sentence = translate_sentence(\n",
    "#         model, sentence, german, english, device, max_length=50\n",
    "#     )\n",
    "\n",
    "#     print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
    "    \n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, sen_batch in enumerate(zip(train_de_loader, train_en_loader)):\n",
    "        source_sentence = sen_batch[0]\n",
    "        target_sentence = sen_batch[1]\n",
    "        #print(target_sentence.shape)\n",
    "        \n",
    "        # Forward\n",
    "        score = model(source_sentence, target_sentence)\n",
    "        score = score[1:].reshape(-1, score.shape[2])\n",
    "        target_sentence = target_sentence[1:].reshape(-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(score, target_sentence)\n",
    "\n",
    "        # Back prop\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
    "        # within a healthy range\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Gradient descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Plot to tensorboard\n",
    "        writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-alfred",
   "metadata": {},
   "source": [
    "## Translation (Prediction)\n",
    "\n",
    "+ tokenizer\n",
    "\"Es ist ein Er.\" -> \"es ist ein er .\"\n",
    "+ numericalizer and format\n",
    "es ist ein er -> [0, 1, 23, 345, 456, 789, 1]\n",
    "+ put into trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-scroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "import torch.data.vocab import Vocab\n",
    "\n",
    "def translation(model, sentence):\n",
    "    en_dictionary = spacy('en_core_webs_sm')\n",
    "    de_tokenizer = get_tokenizer('spacy', 'en')\n",
    "    counter = Counter()\n",
    "    for _s in sentence:\n",
    "        counter.update(de_tokenizer(_s))\n",
    "    Vocab(counter, specialize=)\n",
    "    \n",
    "    numericalize_sentence = ['<BOS>']\n",
    "    numericalize_sentence += Vocab.stoi(sentence)\n",
    "    numericalize_sentence.append('<EOS>')\n",
    "    \n",
    "    model(numericalize_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "surprising-benjamin",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'german' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-173b18bd18c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mtranslate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgerman\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menglish\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'german' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import spacy\n",
    "from torchtext.data.metrics import bleu_score\n",
    "import sys\n",
    "\n",
    "\n",
    "def translate_sentence(model, sentence, german, english, device, max_length=50):\n",
    "    # Load german tokenizer\n",
    "    spacy_ger = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n",
    "    if type(sentence) == str:\n",
    "        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    # Add <SOS> and <EOS> in beginning and end respectively\n",
    "    tokens.insert(0, german.init_token)\n",
    "    tokens.append(german.eos_token)\n",
    "\n",
    "    # Go through each german token and convert to an index\n",
    "    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    # Convert to Tensor\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
    "\n",
    "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
    "    for i in range(max_length):\n",
    "        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(sentence_tensor, trg_tensor)\n",
    "\n",
    "        best_guess = output.argmax(2)[-1, :].item()\n",
    "        outputs.append(best_guess)\n",
    "\n",
    "        if best_guess == english.vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
    "    # remove start token\n",
    "    return translated_sentence[1:]\n",
    "\n",
    "\n",
    "    \n",
    "translate_sentence(model, sentence, german, english, device, max_length=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
